# LLM 委员会

LLM 委员会是一个创新的多模型协作系统，旨在通过让多个大型语言模型共同参与讨论来提升回答质量。该系统将您的查询同时发送给多个 LLM，让它们互相评审并排序彼此的工作，最终由主席模型生成综合性的最终回答。

## 功能特点

1. **多模型协同**：支持数十个大模型同时参与讨论
2. **交叉验证机制**：每个模型对其他模型进行匿名评审，显著提高回答准确性和质量，大幅减少大模型幻觉
3. **透明讨论过程**：可查看每轮讨论的会议结论，节省时间
4. **冗余设计**：灵活配置参会模型数量和冗余机制，确保结果可靠性
5. **开放兼容**：支持多种 API 格式和服务提供商

## 使用方法

### 1. 配置模型

编辑 `config/config.yaml` 文件来自定义委员会配置：

```yaml
llms:
  - model: "Qwen3-Next-80B-A3B-Instruct"
    base_url: "http://localhost:8000/v1"
    api_key: "xxx"

  - model: "Qwen3-30B-A3B-Instruct"
    base_url: "http://localhost:8001/v1"
    api_key: "xxx"
```

### 2. 运行程序

```bash
go run main.go
```

这将执行完整的委员会流程：
- 第一阶段：收集各模型的初步意见
- 第二阶段：各模型匿名评审并排名
- 第三阶段：主席模型整合所有反馈并生成最终答案

## 工作流程

### 第一阶段：初步意见
用户问题被分别发送给所有 LLM，收集各自的回复。

### 第二阶段：评审
每个 LLM 都能看到其他 LLM 的回复。在后台，LLM 身份被匿名化，避免偏袒。LLM 根据准确性和洞察力对彼此进行排名。

### 第三阶段：最终回答
指定的 LLM 委员会主席将所有模型的回复整合成最终答案并呈现给用户。

## 最佳实践

1. **模型选择**：根据任务复杂度合理选择参与的模型数量，避免资源浪费
2. **参数调优**：针对不同的任务类型调整模型的 temperature 和 top_p 参数
3. **错误处理**：当某个模型响应失败时，系统会自动跳过并继续其他模型的处理
4. **资源管理**：对于大量并发请求，建议配置适当的超时时间和重试机制

## 常见问题

### 如何添加新的模型？
在 `config/config.yaml` 中添加新的模型配置项即可。

### 系统如何保证评审的公平性？
所有模型在评审过程中身份都是匿名的，避免任何形式的偏袒。